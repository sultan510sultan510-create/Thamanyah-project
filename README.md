# 🎯 مشروع بث بيانات تفاعل المستخدمين – تنفيذ تكليف مهندس بيانات أول

هذا المشروع هو تنفيذ عملي لتكليف "مهندس بيانات أول"، ويهدف إلى إنشاء نظام بث لحظي لفعاليات تفاعل المستخدمين انطلاقًا من قاعدة بيانات PostgreSQL إلى ثلاث وجهات مختلفة:

1. **BigQuery**: للتحليل وإنشاء التقارير.
2. **Redis**: لمعالجة التفاعلات الزمنية اللحظية (Real-Time).
3. **نظام خارجي**: كمثال على التكامل مع أنظمة خارجية.

---

## ✅ المتطلبات التي تم تنفيذها

| المتطلب | ✅ الحالة |
|---------|-----------|
| بث بيانات من PostgreSQL | ✔️ |
| إثراء وتحويل البيانات | ✔️ |
| إرسال البيانات إلى BigQuery | ✔️ (عبر ملفات JSON) |
| إرسال البيانات إلى Redis | ✔️ |
| إرسال البيانات إلى نظام خارجي | ✔️ |
| زمن التحديث أقل من 5 ثوانٍ | ✔️ |
| دعم إعادة المعالجة (backfill) | ✔️ |
| بيئة Docker قابلة للتكرار | ✔️ |
| توثيق كامل شامل | ✔️ |

---

## 🧱 مكونات النظام بالتفصيل

### 📦 1. `data_generator` – توليد البيانات
- يملأ جدول `content` بمحتوى تجريبي إن كان فارغًا.
- يولد أحداث تفاعل عشوائية (`play`, `pause`, `click`, `finish`).
- يربط كل حدث بمحتوى ومستخدم وجهاز ومدة، ويخزنها في PostgreSQL.

---

### 📦 2. `producer` – القراءة من PostgreSQL والبث إلى Kafka
- يقرأ الأحداث الجديدة مع ربط بيانات المحتوى.
- يحسب `engagement_seconds` و `engagement_pct`.
- يمنع التكرار باستخدام Redis.
- يحفظ توقيت آخر حدث تمت معالجته في `last_processed.txt`.
- يرسل البيانات إلى Kafka.
- يخزن نسخة منها بصيغة JSON في `bigquery_batches/`.

> **لماذا استخدمت تخزين JSON؟**  
> لأن حساب GCP غير مفوتر، فلا يمكن البث المباشر إلى BigQuery. لذا تم تخزين الملفات كـ batches مؤقتة ورفعها لاحقًا عند الحاجة.

> **يدعم أيضًا** تشغيل الوضع التاريخي `backfill` عبر:
```bash
--backfill --since "2024-08-01T00:00:00"
```

---

### 📦 3. `consumer` – استهلاك من Kafka وتوزيع للوجهات
- يستهلك كل رسالة من Kafka ويحوّل التواريخ.
- يرسل المحتوى إلى Redis لتجميع:
  - التفاعل حسب نوع المحتوى.
  - أكثر الأجهزة استخدامًا.
- يحذف الأحداث الأقدم من 10 دقائق.
- يحسب زمن التأخير (Latency).
- يرسل الحدث إلى نظام خارجي عبر API (httpbin).
- يحتوي على دالة `write_to_bq` معلّقة مؤقتًا.

---

### 📦 4. `uploader` – تحميل بيانات JSON إلى BigQuery
- يبحث عن ملفات `.json` في `bigquery_batches/`.
- يتحقق من صلاحية كل سطر JSON.
- ينشئ نسخة نظيفة ويرفعها إلى BigQuery.
- يحذف الملفات بعد الرفع.
- يعمل كل 5 ثوانٍ في حلقة مستمرة.

---

## ⚙️ Docker – إعداد البيئة

ملف `docker-compose.yml` يحتوي على:

- PostgreSQL
- Redis
- Zookeeper + Kafka
- حاويات: `generator`, `producer`, `consumer`, `bigquery`

> **كل مجلد** مرتبط كـ volume لسهولة التطوير والمراقبة.

---

## ▶️ خطوات التشغيل

```bash
docker-compose up --build

# توليد البيانات
docker exec -it generator python data_generator.py

# المعالجة والبث
docker exec -it producer python event_processor_producer.py

# المستهلك (Redis + API خارجي)
docker exec -it consumer python event_processor_consumer.py

# الرفع إلى BigQuery (اختياري)
docker exec -it bigquery python load_to_bigquery.py
```

---

## 🧠 الخلاصة

تم تنفيذ المشروع وفق المتطلبات الكاملة للتكليف، مع الالتزام بجميع الشروط الفنية:
- بث لحظي وتحويلات دقيقة.
- Redis بتأخير أقل من 5 ثوانٍ.
- دعم backfill مرن.
- تخزين مرحلي لـ BigQuery.
- فصل المهام لتوسعة أسهل.
- بيئة Docker متكاملة.
- توثيق شامل لكل خطوة تم تنفيذها.
